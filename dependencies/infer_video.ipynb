{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801bec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from mxnet.contrib.onnx import import_model\n",
    "import cityscapes_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34fdc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(im):\n",
    "    # Convert to float32 for further calculations\n",
    "    rgb_mean = cv.mean(im)\n",
    "    result_shape = [im.shape[0],im.shape[1]]\n",
    "    test_img = im.astype(np.float32)\n",
    "    # Extrapolate image with a small border in order obtain an accurate reshaped image after DUC layer\n",
    "    #first we will get the h and w of the im\n",
    "    test_shape = [im.shape[0],im.shape[1]]\n",
    "    #Determine the cell_shapes by taking each dimension in test_shape, dividing it by 8, and then rounding up to the nearest multiple of 8. This step is likely related to some specific requirements of the subsequent processing.\n",
    "    cell_shapes = [math.ceil(l / 8)*8 for l in test_shape]\n",
    "    #Extend the border of the image (test_img) using cv.copyMakeBorder to match the size specified by cell_shapes. This step adds a border to the image if the calculated size is larger than the original image size, using a constant value (rgb_mean) to fill the border pixels.\n",
    "    test_img = cv.copyMakeBorder(test_img, 0, max(0, int(cell_shapes[0]) - im.shape[0]), 0, max(0, int(cell_shapes[1]) - im.shape[1]), cv.BORDER_CONSTANT, value=rgb_mean)\n",
    "    #Transpose the image dimensions using np.transpose to change the order of the axes. It converts the shape from (height, width, channels) to (channels, height, width).\n",
    "    test_img = np.transpose(test_img, (2, 0, 1))\n",
    "    # Subtract the mean RGB values (rgb_mean) from each channel of the image. It iterates over each channel and subtracts the corresponding mean value.\n",
    "    for i in range(3):\n",
    "        test_img[i] -= rgb_mean[i]\n",
    "    #Expand the dimensions of the image using np.expand_dims to add an extra dimension at the beginning. This is typically done to match the input shape expected by the subsequent processing steps.\n",
    "    test_img = np.expand_dims(test_img, axis=0)\n",
    "    # Convert the image to an mx.ndarray.array using mx.ndarray.array(test_img). This step likely converts the image to a format compatible with the MXNet deep learning framework.\n",
    "    test_img = mx.ndarray.array(test_img)\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbb0c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_palette():\n",
    "    # Get train id to color mappings from file\n",
    "    trainId2colors = {label.trainId: label.color for label in cityscapes_labels.labels}\n",
    "    # Prepare and return palette\n",
    "    palette = [0] * 256 * 3\n",
    "    for trainId in trainId2colors:\n",
    "        colors = trainId2colors[trainId]\n",
    "        if trainId == 255:\n",
    "            colors = (0, 0, 0)\n",
    "        for i in range(3):\n",
    "            palette[trainId * 3 + i] = colors[i]\n",
    "    return palette\n",
    "\n",
    "def colorize(labels):\n",
    "    # Generate colorized image from output labels and color palette\n",
    "    result_img = Image.fromarray(labels).convert('P')\n",
    "    result_img.putpalette(get_palette())\n",
    "    return np.array(result_img.convert('RGB'))\n",
    "\n",
    "def predict(imgs,im):\n",
    "    # Get input and output dimensions\n",
    "    rgb_mean = cv.mean(im)\n",
    "    result_shape = [im.shape[0],im.shape[1]]\n",
    "    result_height, result_width = result_shape\n",
    "    _, _, img_height, img_width = imgs.shape\n",
    "    # Set downsampling rate\n",
    "    ds_rate = 8\n",
    "    # Set cell width\n",
    "    cell_width = 2\n",
    "    # Number of output label classes\n",
    "    label_num = 19\n",
    "    \n",
    "    # Perform forward pass\n",
    "    batch = namedtuple('Batch', ['data'])\n",
    "    mod.forward(batch([imgs]), is_train=False)\n",
    "    labels = mod.get_outputs()[0].asnumpy().squeeze()\n",
    "\n",
    "    # Re-arrange output\n",
    "    test_width = int((int(img_width) / ds_rate) * ds_rate)\n",
    "    test_height = int((int(img_height) / ds_rate) * ds_rate)\n",
    "    feat_width = int(test_width / ds_rate)\n",
    "    feat_height = int(test_height / ds_rate)\n",
    "    labels = labels.reshape((label_num, 4, 4, feat_height, feat_width))\n",
    "    labels = np.transpose(labels, (0, 3, 1, 4, 2))\n",
    "    labels = labels.reshape((label_num, int(test_height / cell_width), int(test_width / cell_width)))\n",
    "\n",
    "    labels = labels[:, :int(img_height / cell_width), :int(img_width / cell_width)]\n",
    "    labels = np.transpose(labels, [1, 2, 0])\n",
    "    labels = cv.resize(labels, (result_width, result_height), interpolation=cv.INTER_LINEAR)\n",
    "    labels = np.transpose(labels, [2, 0, 1])\n",
    "    \n",
    "    # Get softmax output\n",
    "    softmax = labels\n",
    "    \n",
    "    # Get classification labels\n",
    "    results = np.argmax(labels, axis=0).astype(np.uint8)\n",
    "    raw_labels = results\n",
    "\n",
    "    # Compute confidence score\n",
    "    confidence = float(np.max(softmax, axis=0).mean())\n",
    "\n",
    "    # Generate segmented image\n",
    "    result_img = Image.fromarray(colorize(raw_labels)).resize(result_shape[::-1])\n",
    "    \n",
    "    # Generate blended image\n",
    "    blended_img = Image.fromarray(cv.addWeighted(im[:, :, ::-1], 0.5, np.array(result_img), 0.5, 0))\n",
    "\n",
    "    return confidence, result_img, blended_img, raw_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e8e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ctx, model_path,im):\n",
    "    # Import ONNX model into MXNet symbols and params\n",
    "    sym, arg, aux = import_model(model_path)\n",
    "    # Define network module\n",
    "    mod = mx.mod.Module(symbol=sym, data_names=['data'], context=ctx, label_names=None)\n",
    "    # Bind parameters to the network\n",
    "    mod.bind(for_training=False, data_shapes=[('data', (1, 3, im.shape[0], im.shape[1]))], label_shapes=mod._label_shapes)\n",
    "    mod.set_params(arg_params=arg, aux_params=aux, allow_missing=True, allow_extra=True)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06af0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine and set context\n",
    "if len(mx.test_utils.list_gpus())==0:\n",
    "    ctx = mx.cpu()\n",
    "else:\n",
    "    ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b227dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture('video.mp4')\n",
    "ret, frame = cap.read()\n",
    "im = frame[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f5c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling mxnet.contrib.onnx.import_model...\n",
      "Please be advised that importing ONNX models into MXNet is going to be deprecated in the upcoming MXNet v1.10 release. The following apis will be deleted: mxnet.contrib.onnx.import_model/get_model_metadata/import_to_gluon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:44:25] ../src/executor/graph_executor.cc:1991: Subgraph backend MKLDNN is activated.\n"
     ]
    }
   ],
   "source": [
    "# Load ONNX model\n",
    "mod = get_model(ctx, 'ResNet101_DUC_HDC.onnx',im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e942fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test video\n",
    "# mx.test_utils.download('https://example.com/video.mp4', fname='video.mp4')\n",
    "\n",
    "# Read video and initialize variables\n",
    "cap = cv.VideoCapture('video.mp4')\n",
    "frame_count = 0\n",
    "frame_skip = 5\n",
    "\n",
    "# Iterate over video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply frame skipping logic\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_skip != 0:\n",
    "        continue\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    im = frame[:, :, ::-1]\n",
    "    pre = preprocess(im)\n",
    "    conf, result_img, blended_img, raw = predict(pre,im)\n",
    "\n",
    "    # Display or save the results as desired\n",
    "    cv.imshow('Result', np.array(result_img))\n",
    "    cv.imshow('Blended', np.array(blended_img))\n",
    "    \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63aa71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
